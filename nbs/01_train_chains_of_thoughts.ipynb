{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12841fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from llama_4bit_wrapper import import_llama\n",
    "from llama_4bit_wrapper.core import Matmul4BitOptions\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0e08f",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19448eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>variables</th>\n",
       "      <th>target</th>\n",
       "      <th>dataset</th>\n",
       "      <th>formatter</th>\n",
       "      <th>subset</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This question refers to the following informat...</td>\n",
       "      <td>{'A': 'The ideas of personal liberty and natio...</td>\n",
       "      <td>A</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu-high_school_european_history</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This question refers to the following informat...</td>\n",
       "      <td>{'A': 'Capitalist', 'B': 'Scientific', 'C': 'C...</td>\n",
       "      <td>C</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu-high_school_european_history</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This question refers to the following informat...</td>\n",
       "      <td>{'A': 'They served as a catalyst for the growt...</td>\n",
       "      <td>A</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu-high_school_european_history</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This question refers to the following informat...</td>\n",
       "      <td>{'A': 'give the English king a new position of...</td>\n",
       "      <td>D</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu-high_school_european_history</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This question refers to the following informat...</td>\n",
       "      <td>{'A': 'His domination of the nobility left him...</td>\n",
       "      <td>D</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu-high_school_european_history</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  This question refers to the following informat...   \n",
       "1  This question refers to the following informat...   \n",
       "2  This question refers to the following informat...   \n",
       "3  This question refers to the following informat...   \n",
       "4  This question refers to the following informat...   \n",
       "\n",
       "                                           variables target dataset formatter  \\\n",
       "0  {'A': 'The ideas of personal liberty and natio...      A    mmlu      mmlu   \n",
       "1  {'A': 'Capitalist', 'B': 'Scientific', 'C': 'C...      C    mmlu      mmlu   \n",
       "2  {'A': 'They served as a catalyst for the growt...      A    mmlu      mmlu   \n",
       "3  {'A': 'give the English king a new position of...      D    mmlu      mmlu   \n",
       "4  {'A': 'His domination of the nobility left him...      D    mmlu      mmlu   \n",
       "\n",
       "                              subset  split  \n",
       "0  mmlu-high_school_european_history  train  \n",
       "1  mmlu-high_school_european_history  train  \n",
       "2  mmlu-high_school_european_history  train  \n",
       "3  mmlu-high_school_european_history  train  \n",
       "4  mmlu-high_school_european_history   test  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"llama-reasoning.csv\")\n",
    "df_data[\"variables\"] = df_data[\"variables\"].apply(json.loads)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74ca326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>variables</th>\n",
       "      <th>target</th>\n",
       "      <th>dataset</th>\n",
       "      <th>formatter</th>\n",
       "      <th>subset</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>Artemis is making tea for a party. She knows h...</td>\n",
       "      <td>{'chain_of_thoughts': 'She is making 72 ounces...</td>\n",
       "      <td>9</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>It's Ava's birthday party. Her parents bought ...</td>\n",
       "      <td>{'chain_of_thoughts': 'The four bags of Reese'...</td>\n",
       "      <td>99</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>Lee mows one lawn and charges $33. Last week h...</td>\n",
       "      <td>{'chain_of_thoughts': '33 * 16 = $&lt;&lt;33*16=528&gt;...</td>\n",
       "      <td>558</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>Carly collected 7 starfish with 5 arms each an...</td>\n",
       "      <td>{'chain_of_thoughts': 'First find the total nu...</td>\n",
       "      <td>49</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>Hannah sold 40 pieces of cookies for $0.8 each...</td>\n",
       "      <td>{'chain_of_thoughts': 'Hannah's earnings from ...</td>\n",
       "      <td>79</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>Bud makes homemade macaroni and cheese once a ...</td>\n",
       "      <td>{'chain_of_thoughts': 'The gruyere cheese is t...</td>\n",
       "      <td>520</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>A farm has 10 2-legged animals and 15 4-legged...</td>\n",
       "      <td>{'chain_of_thoughts': '2-legged animals have 2...</td>\n",
       "      <td>40</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>The recent floods in Mamou’s country have left...</td>\n",
       "      <td>{'chain_of_thoughts': 'On Friday, Saturday and...</td>\n",
       "      <td>1218</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>Jake is shopping at a clothing store. The stor...</td>\n",
       "      <td>{'chain_of_thoughts': 'The cost of a T-shirt a...</td>\n",
       "      <td>36</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>Janet goes to the mall and spends $3.50 on ice...</td>\n",
       "      <td>{'chain_of_thoughts': 'First calculate the cos...</td>\n",
       "      <td>13</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>gsm8k</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "828   Artemis is making tea for a party. She knows h...   \n",
       "829   It's Ava's birthday party. Her parents bought ...   \n",
       "830   Lee mows one lawn and charges $33. Last week h...   \n",
       "831   Carly collected 7 starfish with 5 arms each an...   \n",
       "832   Hannah sold 40 pieces of cookies for $0.8 each...   \n",
       "...                                                 ...   \n",
       "1123  Bud makes homemade macaroni and cheese once a ...   \n",
       "1124  A farm has 10 2-legged animals and 15 4-legged...   \n",
       "1125  The recent floods in Mamou’s country have left...   \n",
       "1126  Jake is shopping at a clothing store. The stor...   \n",
       "1127  Janet goes to the mall and spends $3.50 on ice...   \n",
       "\n",
       "                                              variables target dataset  \\\n",
       "828   {'chain_of_thoughts': 'She is making 72 ounces...      9   gsm8k   \n",
       "829   {'chain_of_thoughts': 'The four bags of Reese'...     99   gsm8k   \n",
       "830   {'chain_of_thoughts': '33 * 16 = $<<33*16=528>...    558   gsm8k   \n",
       "831   {'chain_of_thoughts': 'First find the total nu...     49   gsm8k   \n",
       "832   {'chain_of_thoughts': 'Hannah's earnings from ...     79   gsm8k   \n",
       "...                                                 ...    ...     ...   \n",
       "1123  {'chain_of_thoughts': 'The gruyere cheese is t...    520   gsm8k   \n",
       "1124  {'chain_of_thoughts': '2-legged animals have 2...     40   gsm8k   \n",
       "1125  {'chain_of_thoughts': 'On Friday, Saturday and...   1218   gsm8k   \n",
       "1126  {'chain_of_thoughts': 'The cost of a T-shirt a...     36   gsm8k   \n",
       "1127  {'chain_of_thoughts': 'First calculate the cos...     13   gsm8k   \n",
       "\n",
       "     formatter subset       split  \n",
       "828      gsm8k  gsm8k  validation  \n",
       "829      gsm8k  gsm8k       train  \n",
       "830      gsm8k  gsm8k  validation  \n",
       "831      gsm8k  gsm8k       train  \n",
       "832      gsm8k  gsm8k       train  \n",
       "...        ...    ...         ...  \n",
       "1123     gsm8k  gsm8k        test  \n",
       "1124     gsm8k  gsm8k        test  \n",
       "1125     gsm8k  gsm8k        test  \n",
       "1126     gsm8k  gsm8k        test  \n",
       "1127     gsm8k  gsm8k        test  \n",
       "\n",
       "[300 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.loc[df_data[\"dataset\"] == \"gsm8k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7314e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formatter</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bbh-boolean_expressions</td>\n",
       "      <td>USER: Calculate the following expression {ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bbh-boolean_expressions-no-cot</td>\n",
       "      <td>USER: Calculate the following expression {ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbh-boolean_expressions-cot</td>\n",
       "      <td>USER: Calculate the following expression {ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bbh-causal_judgement</td>\n",
       "      <td>USER: {question}. &lt;s&gt; ASSISTANT: Let's think s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbh-causal_judgement-cot</td>\n",
       "      <td>USER: {question}. &lt;s&gt; ASSISTANT: Let's think s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        formatter  \\\n",
       "0         bbh-boolean_expressions   \n",
       "1  bbh-boolean_expressions-no-cot   \n",
       "2     bbh-boolean_expressions-cot   \n",
       "3            bbh-causal_judgement   \n",
       "4        bbh-causal_judgement-cot   \n",
       "\n",
       "                                                text  \n",
       "0  USER: Calculate the following expression {ques...  \n",
       "1  USER: Calculate the following expression {ques...  \n",
       "2  USER: Calculate the following expression {ques...  \n",
       "3  USER: {question}. <s> ASSISTANT: Let's think s...  \n",
       "4  USER: {question}. <s> ASSISTANT: Let's think s...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_formatters = pd.read_csv(\"formatters.csv\")\n",
    "df_formatters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586bdb5",
   "metadata": {},
   "source": [
    "## Applying formatters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24827d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_apply_formatters(row, df_formatters, suffix):\n",
    "    pattern = df_formatters.loc[df_formatters[\"formatter\"] == row[\"formatter\"] + suffix, \"text\"].values[0]\n",
    "    data = dict(row)\n",
    "    data = dict(data, **row[\"variables\"])\n",
    "    return pattern.format(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d7e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_formatters(df_data, df_formatters, suffix):\n",
    "    return df_data.apply(\n",
    "        lambda row: row_apply_formatters(row, df_formatters, suffix),\n",
    "        axis=1\n",
    "    ).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f82fc5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question\n",
       "split               \n",
       "test             565\n",
       "train            479\n",
       "validation       548"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.groupby(\"split\")[[\"question\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf269f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USER: This question refers to the following information.\\nRead the following excerpt.\\nThe revolutionary seed had penetrated into every country and spread more or less. It was greatly developed under the régime of the military despotism of Bonaparte. His conquests displaced a number of laws, institutions, and customs; broke through bonds sacred among all nations, strong enough to resist time itself; which is more than can be said of certain benefits conferred by these innovators.\\nThe monarchs will fulfil the duties imposed upon them by Him who, by entrusting them with power, has charged them to watch over the maintenance of justice, and the rights of all, to avoid the paths of error, and tread firmly in the way of truth. Placed beyond the passions which agitate society, it is in days of trial chiefly that they are called upon to despoil realities of their false appearances, and to show themselves as they are, fathers invested with the authority belonging by right to the heads of families, to prove that, in days of mourning, they know how to be just, wise, and therefore strong, and that they will not abandon the people whom they ought to govern to be the sport of factions, to error and its consequences, which must involve the loss of society.\\nUnion between the monarchs is the basis of the policy which must now be followed to save society from total ruin. . . .\\nLet them not confound concessions made to parties with the good they ought to do for their people, in modifying, according to their recognized needs, such branches of the administration as require it.\\nLet them be just, but strong; beneficent, but strict.\\nLet them maintain religious principles in all their purity, and not allow the faith to be attacked and morality interpreted according to the social contract or the visions of foolish sectarians.\\nLet them suppress Secret Societies; that gangrene of society.\\n—Klemens von Metternich, Political Confession of Faith, 1820\\nWhich of the following was the greatest cause of the fears expressed by Metternich in the document above?\\nA: The ideas of personal liberty and nationalism conceived during the Enlightenment resulted in radical revolutions that could spread throughout Europe.\\nB: The conquest of Europe by Napoleon led to the creation of new factions and shifted the European balance of power.\\nC: The power of monarchs had grown to the point where it needed to be checked by other powers within each nation or domination of civilians would occur.\\nD: The rising and falling economic cycle of the newly emerging capitalist economy could lead to civilian unrest that must be suppressed.\\nGive an immediate answer. <s> ASSISTANT: A <s>',\n",
       " 'USER: This question refers to the following information.\\nIn Russia there was nothing going on well, and [Souvarine] was in despair over the news he had received. His old companions were all turning to the politicians; the famous Nihilists who made Europe tremble-sons of village priests, of the lower middle class, of tradesmen-could not rise above the idea of national liberation, and seemed to believe that the world would be delivered-when they had killed their despot&…\\n\"Foolery! They\\'ll never get out of it with their foolery.\"\\nThen, lowering his voice still more, in a few bitter words he described his old dream of fraternity. He had renounced his rank and his fortune; he had gone among workmen, only in the hope of seeing at last the foundation of a new society of labour in common. All the sous in his pockets had long gone to the urchins of the settlement; he had been as tender as a brother with the colliers, smiling at their suspicion, winning them over by his quiet workmanlike ways and his dislike of chattering. But decidedly the fusion had not taken place.\\nHis voice changed, his eyes grew bright, he fixed them on étienne, directly addressing him:\\n\"Now, do you understand that? These hatworkers at Marseilles who have won the great lottery prize of a hundred thousand francs have gone off at once and invested it, declaring that they are going to live without doing anything! Yes, that is your idea, all of you French workmen; you want to unearth a treasure in order to devour it alone afterwards in some lazy, selfish corner. You may cry out as much as you like against the rich, you haven\\'t got courage enough to give back to the poor the money that luck brings you. You will never be worthy of happiness as long as you own anything, and your hatred of the bourgeois proceeds solely from an angry desire to be bourgeois yourselves in their place.\"\\némile Zola, French writer, Germinal, 1885\\nThe passage displays the direct concern for the welfare of the working classes that was typically a part of which movement?\\nA: Capitalist\\nB: Scientific\\nC: Communist\\nD: Existentialist\\nGive an immediate answer. <s> ASSISTANT: C <s>',\n",
       " \"USER: This question refers to the following information.\\nThe excerpts below are from the Navigation Acts of 1651.\\n[A]fter the first day of December, one thousand six hundred fifty and one, and from thence forwards, no goods or commodities whatsoever of the growth, production or manufacture of Asia, Africa or America, or of any part thereof; or of any islands belonging to them, or which are described or laid down in the usual maps or cards of those places, as well of the English plantations as others, shall be imported or brought into this Commonwealth of England, or into Ireland, or any other lands, islands, plantations, or territories to this Commonwealth belonging, or in their possession, in any other ship or ships, vessel or vessels whatsoever, but only in such as do truly and without fraud belong only to the people of this Commonwealth, or the plantations thereof, as the proprietors or right owners thereof; and whereof the master and mariners are also of the people of this Commonwealth, under the penalty of the forfeiture and loss of all the goods that shall be imported contrary to this act, , , ,\\n[N]o goods or commodities of the growth, production, or manufacture of Europe, or of any part thereof, shall after the first day of December, one thousand six hundred fifty and one, be imported or brought into this Commonwealth of England, or any other lands or territories to this Commonwealth belonging, or in their possession, in any ship or ships, vessel or vessels whatsoever, but in such as do truly and without fraud belong only to the people of this Commonwealth, and in no other, except only such foreign ships and vessels as do truly and properly belong to the people of that country or place, of which the said goods are the growth, production or manufacture.\\nWhich of the following best describes the outcome of the Navigation Acts of 1651?\\nA: They served as a catalyst for the growth of English shipping and overseas trade, but did little to limit the prospects of the Dutch in the seventeenth century.\\nB: They brought about almost immediate hardships for the Dutch economy as their dominance of overseas trade quickly ended.\\nC: They were rescinded during the restoration of the Stuarts as they sought normal diplomatic relations with the Dutch so not as to need Parliament's financial support for war.\\nD: They led to nearly a century of recurrent war between England and the Netherlands, which would not end until after American independence.\\nGive an immediate answer. <s> ASSISTANT: A <s>\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_formatters(\n",
    "    df_data,\n",
    "    df_formatters,\n",
    "    \"-no-cot\"\n",
    ")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56de36a",
   "metadata": {},
   "source": [
    "## Searching for last \"ASSISTANT: \" answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452077d",
   "metadata": {},
   "source": [
    "To calculate perplexity only on the final answer I should find this answer in the prompt, so I wrote the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff889a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_subsequence_start(sequence, subsequence):\n",
    "    seq_len = len(sequence)\n",
    "    subseq_len = len(subsequence)\n",
    "\n",
    "    for i in range(seq_len - 1, subseq_len - 2, -1):\n",
    "        if sequence[i] == subsequence[-1]:\n",
    "            match = True\n",
    "            for j in range(subseq_len):\n",
    "                if sequence[i - j] != subsequence[-(j + 1)]:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                return i - subseq_len + 1\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37c1f292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_subsequence_start([1, 2, 3, 4, 2, 3, 5], [2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb01c5",
   "metadata": {},
   "source": [
    "## Loading LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56634f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_MODEL = \"./Llama-2-13B-chat-GPTQ-localmodels/\"\n",
    "LLAMA_WEIGHTS = \"./Llama-2-13B-chat-GPTQ-localmodels/gptq_model-4bit-128g.safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c7c21c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton not found. Please run \"pip install triton\".\n",
      "Using CUDA implementation.\n"
     ]
    }
   ],
   "source": [
    "_, _, load_llama_model_4bit_low_ram, _, model_to_half, _, _, _, AMPWrapper = import_llama(\n",
    "    use_flash_attention=False,\n",
    "    use_xformers=False,\n",
    "    autograd_4bit_cuda=True,\n",
    "    autograd_4bit_triton=False,\n",
    "    matmul4bit_options=Matmul4BitOptions.NO_ACT_ORDER | Matmul4BitOptions.ALGORYTHM_FASTER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2302a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at ./Llama-2-13B-chat-GPTQ-localmodels/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted as Half.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 6.83 seconds.\n"
     ]
    }
   ],
   "source": [
    "llama, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=LLAMA_MODEL,\n",
    "    model_path=LLAMA_WEIGHTS,\n",
    "    groupsize=128,\n",
    "    half=True,\n",
    "    device_map='auto',\n",
    "    seqlen=2048,\n",
    "    is_v1_model=False,\n",
    "    bits=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41a25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5090033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_wrapper = AMPWrapper(llama)\n",
    "amp_wrapper.apply_forward()\n",
    "amp_wrapper.apply_generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7291b88",
   "metadata": {},
   "source": [
    "## Checking prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87f224b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_assistant_response_tokens = tokenizer.encode(\"ASSISTANT:\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fa15d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁A'] 319\n",
      "['SS'] 1799\n",
      "['IST'] 9047\n",
      "['ANT'] 13566\n",
      "[':'] 29901\n"
     ]
    }
   ],
   "source": [
    "for token in last_assistant_response_tokens:\n",
    "    print(tokenizer.convert_ids_to_tokens([token]), token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "082cfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([\n",
    "    last_subsequence_start(tokenizer.encode(text), last_assistant_response_tokens) != -1\n",
    "    for text in apply_formatters(\n",
    "        df_data,\n",
    "        df_formatters,\n",
    "        \"-no-cot\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "674b6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([\n",
    "    last_subsequence_start(tokenizer.encode(text), last_assistant_response_tokens) != -1\n",
    "    for text in apply_formatters(\n",
    "        df_data,\n",
    "        df_formatters,\n",
    "        \"\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c8aa5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([\n",
    "    last_subsequence_start(tokenizer.encode(text), last_assistant_response_tokens) != -1\n",
    "    for text in apply_formatters(\n",
    "        df_data.assign(chain_of_thoughts=\"\"),\n",
    "        df_formatters,\n",
    "        \"-cot\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26a764",
   "metadata": {},
   "source": [
    "## Calculating answers perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b57ba1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length_sorted_dataframe(texts):\n",
    "    df = pd.DataFrame({\n",
    "        \"text\": texts,\n",
    "    })\n",
    "    df[\"length\"] = df[\"text\"].str.len()\n",
    "    df[\"index\"] = list(range(len(texts)))\n",
    "    df = df.sort_values(\"length\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "056040b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_perplexities(llama, tokenizer, batch_texts, last_assistant_response_tokens):\n",
    "    batch_size = len(batch_texts)\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        batch_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    batch = {\n",
    "        key: value.to(llama.device)\n",
    "        for key, value in batch.items()\n",
    "    }\n",
    "    llama.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = llama(**batch).logits\n",
    "        probas = F.softmax(logits, dim=-1)\n",
    "    input_ids_np = batch[\"input_ids\"].detach().cpu().numpy()\n",
    "    attention_mask_np = batch[\"attention_mask\"].detach().cpu().numpy()\n",
    "    probas_np = probas.detach().cpu().numpy()\n",
    "    perplexities = []\n",
    "    for i in range(batch_size):\n",
    "        item_mask = attention_mask_np[i].astype(np.bool_)\n",
    "        item_input_ids = input_ids_np[i][item_mask]\n",
    "        item_probas = probas_np[i][item_mask]\n",
    "        answer_start_token_index = last_subsequence_start(item_input_ids, last_assistant_response_tokens)\n",
    "        # Exclude ending <s> token\n",
    "        answer_labels = item_input_ids[answer_start_token_index + len(last_assistant_response_tokens):-1]\n",
    "        answer_probas = item_probas[answer_start_token_index + len(last_assistant_response_tokens) - 1:-2]\n",
    "        answer_token_probas = []\n",
    "        for i, token in enumerate(answer_labels):\n",
    "            answer_token_probas.append(answer_probas[i, token])\n",
    "        answer_token_probas = np.array(answer_token_probas)\n",
    "        perplexity = np.exp(-np.log(answer_token_probas).mean())\n",
    "        perplexities.append(perplexity)\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86e63a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers_perplexity(llama, tokenizer, texts, last_assistant_response_tokens, batch_size):\n",
    "    tokenizer.padding_side = 'right'\n",
    "    df = get_text_length_sorted_dataframe(texts)\n",
    "    batch_count = int(np.ceil(len(df) / batch_size))\n",
    "    perplexities = np.zeros([len(texts)], dtype=np.float32)\n",
    "    batch_indices = np.arange(batch_count, dtype=np.int32)\n",
    "    batch_indices = pd.Series(batch_indices).sample(len(batch_indices), random_state=42).values\n",
    "    for batch_index in tqdm(batch_indices):\n",
    "        df_batch = df.iloc[batch_index * batch_size : (batch_index + 1) * batch_size]\n",
    "        batch_real_size = len(df_batch)\n",
    "        texts_batch = df_batch[\"text\"].tolist()\n",
    "        batch_perplexities = get_batch_perplexities(llama, tokenizer, texts_batch, last_assistant_response_tokens)\n",
    "        perplexities[batch_index * batch_size : batch_index * batch_size + batch_real_size] = batch_perplexities\n",
    "    df[\"perplexity\"] = perplexities\n",
    "    df = df.sort_values(\"index\")\n",
    "    return df[\"perplexity\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888093b",
   "metadata": {},
   "source": [
    "## Calculate initial answers perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0adc4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iteration_no_cot_perplexities(step, llama, tokenizer, df_data, df_formatters,\n",
    "                                      last_assistant_response_tokens, batch_size):\n",
    "    df_data = df_data.copy()\n",
    "    fname = f\"cached-{step}-no-cot-perplexities.csv\"\n",
    "    if not os.path.exists(fname):\n",
    "        df_data[\"no_cot_perplexity\"] = get_answers_perplexity(\n",
    "            llama,\n",
    "            tokenizer,\n",
    "            apply_formatters(\n",
    "                df_data,\n",
    "                df_formatters,\n",
    "                \"-no-cot\"\n",
    "            ),\n",
    "            last_assistant_response_tokens,\n",
    "            batch_size,\n",
    "        )\n",
    "        df_data[\"variables\"] = df_data[\"variables\"].apply(json.dumps)\n",
    "        df_data.to_csv(fname, index=False)\n",
    "    df_data = pd.read_csv(fname)\n",
    "    df_data[\"variables\"] = df_data[\"variables\"].apply(json.loads)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfca31cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>variables</th>\n",
       "      <th>target</th>\n",
       "      <th>dataset</th>\n",
       "      <th>formatter</th>\n",
       "      <th>subset</th>\n",
       "      <th>split</th>\n",
       "      <th>no_cot_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This question refers to the following informat...</td>\n",
       "      <td>{'A': 'The ideas of personal liberty and natio...</td>\n",
       "      <td>A</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu-high_school_european_history</td>\n",
       "      <td>train</td>\n",
       "      <td>1444.3512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This question refers to the following informat...</td>\n",
       "      <td>{'A': 'Capitalist', 'B': 'Scientific', 'C': 'C...</td>\n",
       "      <td>C</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu-high_school_european_history</td>\n",
       "      <td>train</td>\n",
       "      <td>2706.0737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This question refers to the following informat...</td>\n",
       "      <td>{'A': 'They served as a catalyst for the growt...</td>\n",
       "      <td>A</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu-high_school_european_history</td>\n",
       "      <td>train</td>\n",
       "      <td>2104.2961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This question refers to the following informat...</td>\n",
       "      <td>{'A': 'give the English king a new position of...</td>\n",
       "      <td>D</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu-high_school_european_history</td>\n",
       "      <td>train</td>\n",
       "      <td>5377.2250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This question refers to the following informat...</td>\n",
       "      <td>{'A': 'His domination of the nobility left him...</td>\n",
       "      <td>D</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu</td>\n",
       "      <td>mmlu-high_school_european_history</td>\n",
       "      <td>test</td>\n",
       "      <td>11120.2450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  This question refers to the following informat...   \n",
       "1  This question refers to the following informat...   \n",
       "2  This question refers to the following informat...   \n",
       "3  This question refers to the following informat...   \n",
       "4  This question refers to the following informat...   \n",
       "\n",
       "                                           variables target dataset formatter  \\\n",
       "0  {'A': 'The ideas of personal liberty and natio...      A    mmlu      mmlu   \n",
       "1  {'A': 'Capitalist', 'B': 'Scientific', 'C': 'C...      C    mmlu      mmlu   \n",
       "2  {'A': 'They served as a catalyst for the growt...      A    mmlu      mmlu   \n",
       "3  {'A': 'give the English king a new position of...      D    mmlu      mmlu   \n",
       "4  {'A': 'His domination of the nobility left him...      D    mmlu      mmlu   \n",
       "\n",
       "                              subset  split  no_cot_perplexity  \n",
       "0  mmlu-high_school_european_history  train          1444.3512  \n",
       "1  mmlu-high_school_european_history  train          2706.0737  \n",
       "2  mmlu-high_school_european_history  train          2104.2961  \n",
       "3  mmlu-high_school_european_history  train          5377.2250  \n",
       "4  mmlu-high_school_european_history   test         11120.2450  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = get_iteration_no_cot_perplexities(\n",
    "    step=0,\n",
    "    llama=llama,\n",
    "    tokenizer=tokenizer,\n",
    "    df_data=df_data,\n",
    "    df_formatters=df_formatters,\n",
    "    last_assistant_response_tokens=last_assistant_response_tokens,\n",
    "    batch_size=4\n",
    ")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9435807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2680.1576499624375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[\"no_cot_perplexity\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef205c",
   "metadata": {},
   "source": [
    "## Generate chain-of-thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bad8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRASTIVE_SEARCH_ALPHA = 0.6\n",
    "CONTRASTIVE_SEARCH_TOP_K = 4\n",
    "COT_GENERATION_MAX_NEW_TOKENS = 64\n",
    "COT_ANSWER_MINIMUM_TOKENS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75db1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_chains_of_thought(llama, tokenizer, batch_texts):\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        batch_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    batch = {\n",
    "        key: value.to(llama.device)\n",
    "        for key, value in batch.items()\n",
    "    }\n",
    "    max_position_embeddings = llama.config.max_position_embeddings\n",
    "    max_sequence_length = batch[\"attention_mask\"].sum(dim=-1).max().item()\n",
    "    max_new_tokens = COT_GENERATION_MAX_NEW_TOKENS\n",
    "    if max_sequence_length + max_new_tokens + COT_ANSWER_MINIMUM_TOKENS > max_position_embeddings:\n",
    "        max_new_tokens = max_position_embeddings - max_sequence_length - COT_ANSWER_MINIMUM_TOKENS\n",
    "    llama.eval()\n",
    "    with torch.no_grad():\n",
    "        generation = llama.generate(batch[\"input_ids\"],\n",
    "                                    penalty_alpha=CONTRASTIVE_SEARCH_ALPHA,\n",
    "                                    top_k=CONTRASTIVE_SEARCH_TOP_K,\n",
    "                                    max_new_tokens=max_new_tokens,\n",
    "                                    use_cache=True)\n",
    "        generation = generation.detach().cpu().numpy()\n",
    "    batch_size = len(batch_texts)\n",
    "    result = []\n",
    "    for i in range(batch_size):\n",
    "        mask = generation[i] != tokenizer.pad_token_id\n",
    "        item_tokens = generation[i][mask]\n",
    "        prompt_token_count = (batch[\"input_ids\"][i] != tokenizer.pad_token_id).sum().item()\n",
    "        item_generation_tokens = item_tokens[prompt_token_count:]\n",
    "        item_generation_text = tokenizer.decode(item_generation_tokens)\n",
    "        item_generation_text_cleaned = item_generation_text.split(\"ASSISTANT:\")[0].split(\"USER:\")[0].strip()\n",
    "        result.append(item_generation_text_cleaned)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9472731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chains_of_thought(llama, tokenizer, texts, batch_size):\n",
    "    tokenizer.padding_side = 'left'\n",
    "    df = get_text_length_sorted_dataframe(texts)\n",
    "    batch_count = int(np.ceil(len(df) / batch_size))\n",
    "    batch_indices = np.arange(batch_count, dtype=np.int32)\n",
    "    batch_indices = pd.Series(batch_indices).sample(len(batch_indices), random_state=42).values\n",
    "    \n",
    "    generations = {}\n",
    "    for batch_index in tqdm(batch_indices):\n",
    "        df_batch = df.iloc[batch_index * batch_size : (batch_index + 1) * batch_size]\n",
    "        batch_real_size = len(df_batch)\n",
    "        texts_batch = df_batch[\"text\"].tolist()\n",
    "        generations_batch = get_batch_chains_of_thought(llama, tokenizer, texts_batch)\n",
    "        for i in range(batch_real_size):\n",
    "            generations[batch_index * batch_size + i] = generations_batch[i]\n",
    "    \n",
    "    df[\"chain_of_thoughts\"] = [\n",
    "        generations[i]\n",
    "        for i in range(len(texts))\n",
    "    ]\n",
    "    df = df.sort_values(\"index\")\n",
    "    return df[\"chain_of_thoughts\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cd17c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iteration_chains_of_thoughts(step, llama, tokenizer, df_data, df_formatters, batch_size):\n",
    "    df_data = df_data.copy()\n",
    "    fname = f\"cached-{step}-chains-of-thoughts.csv\"\n",
    "    if not os.path.exists(fname):\n",
    "        df_data[\"chain_of_thoughts\"] = get_chains_of_thought(\n",
    "            llama,\n",
    "            tokenizer,\n",
    "            apply_formatters(\n",
    "                df_data,\n",
    "                df_formatters,\n",
    "                \"\",\n",
    "            ),\n",
    "            batch_size,\n",
    "        )\n",
    "        df_data[\"variables\"] = df_data[\"variables\"].apply(json.dumps)\n",
    "        df_data.to_csv(fname, index=False)\n",
    "    df_data = pd.read_csv(fname)\n",
    "    df_data[\"variables\"] = df_data[\"variables\"].apply(json.loads)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58c7b81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b03b8d9a23d4fb2ba67d2c1c441bd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 11.00 GiB total capacity; 35.88 GiB already allocated; 0 bytes free; 36.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_data \u001b[38;5;241m=\u001b[39m get_iteration_chains_of_thoughts(\n\u001b[0;32m      2\u001b[0m     step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      3\u001b[0m     llama\u001b[38;5;241m=\u001b[39mllama,\n\u001b[0;32m      4\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m      5\u001b[0m     df_data\u001b[38;5;241m=\u001b[39mdf_data,\n\u001b[0;32m      6\u001b[0m     df_formatters\u001b[38;5;241m=\u001b[39mdf_formatters,\n\u001b[0;32m      7\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m df_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m, in \u001b[0;36mget_iteration_chains_of_thoughts\u001b[1;34m(step, llama, tokenizer, df_data, df_formatters, batch_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcached-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-chains-of-thoughts.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(fname):\n\u001b[1;32m----> 5\u001b[0m     df_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchain_of_thoughts\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_chains_of_thought(\n\u001b[0;32m      6\u001b[0m         llama,\n\u001b[0;32m      7\u001b[0m         tokenizer,\n\u001b[0;32m      8\u001b[0m         apply_formatters(\n\u001b[0;32m      9\u001b[0m             df_data,\n\u001b[0;32m     10\u001b[0m             df_formatters,\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m         ),\n\u001b[0;32m     13\u001b[0m         batch_size,\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     15\u001b[0m     df_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(json\u001b[38;5;241m.\u001b[39mdumps)\n\u001b[0;32m     16\u001b[0m     df_data\u001b[38;5;241m.\u001b[39mto_csv(fname, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[29], line 13\u001b[0m, in \u001b[0;36mget_chains_of_thought\u001b[1;34m(llama, tokenizer, texts, batch_size)\u001b[0m\n\u001b[0;32m     11\u001b[0m batch_real_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_batch)\n\u001b[0;32m     12\u001b[0m texts_batch \u001b[38;5;241m=\u001b[39m df_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 13\u001b[0m generations_batch \u001b[38;5;241m=\u001b[39m get_batch_chains_of_thought(llama, tokenizer, texts_batch)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_real_size):\n\u001b[0;32m     15\u001b[0m     generations[batch_index \u001b[38;5;241m*\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m i] \u001b[38;5;241m=\u001b[39m generations_batch[i]\n",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m, in \u001b[0;36mget_batch_chains_of_thought\u001b[1;34m(llama, tokenizer, batch_texts)\u001b[0m\n\u001b[0;32m     16\u001b[0m llama\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 18\u001b[0m     generation \u001b[38;5;241m=\u001b[39m llama\u001b[38;5;241m.\u001b[39mgenerate(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     19\u001b[0m                                 penalty_alpha\u001b[38;5;241m=\u001b[39mCONTRASTIVE_SEARCH_ALPHA,\n\u001b[0;32m     20\u001b[0m                                 top_k\u001b[38;5;241m=\u001b[39mCONTRASTIVE_SEARCH_TOP_K,\n\u001b[0;32m     21\u001b[0m                                 max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[0;32m     22\u001b[0m                                 use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m     generation \u001b[38;5;241m=\u001b[39m generation\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     24\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_texts)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\alpaca_lora_4bit\\amp_wrapper.py:18\u001b[0m, in \u001b[0;36mAMPWrapper.autocast_generate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautocast_generate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions):\n\u001b[1;32m---> 18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnon_autocast_generate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\transformers\\generation\\utils.py:1560\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1558\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContrastive search requires `use_cache=True`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrastive_search(\n\u001b[0;32m   1561\u001b[0m         input_ids,\n\u001b[0;32m   1562\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mtop_k,\n\u001b[0;32m   1563\u001b[0m         penalty_alpha\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpenalty_alpha,\n\u001b[0;32m   1564\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1565\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1566\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1567\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1568\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1569\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1570\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1571\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1573\u001b[0m     )\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_sample_gen_mode:\n\u001b[0;32m   1576\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\transformers\\generation\\utils.py:2058\u001b[0m, in \u001b[0;36mGenerationMixin.contrastive_search\u001b[1;34m(self, input_ids, top_k, penalty_alpha, logits_processor, logits_warper, stopping_criteria, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2056\u001b[0m \u001b[38;5;66;03m# compute the candidate tokens by the language model and collects their hidden_states\u001b[39;00m\n\u001b[0;32m   2057\u001b[0m next_model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(top_k_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 2058\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2059\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnext_model_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions\n\u001b[0;32m   2060\u001b[0m )\n\u001b[0;32m   2061\u001b[0m next_past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_past_from_model_output(outputs, standardize_cache_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2063\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\alpaca_lora_4bit\\amp_wrapper.py:14\u001b[0m, in \u001b[0;36mAMPWrapper.autocast_forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautocast_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions):\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnon_autocast_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    803\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 806\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    807\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    808\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    809\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    810\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    811\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    812\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    813\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    814\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    815\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    816\u001b[0m )\n\u001b[0;32m    818\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    685\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m    687\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    691\u001b[0m     )\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 693\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    694\u001b[0m         hidden_states,\n\u001b[0;32m    695\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    696\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    697\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    698\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    699\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    700\u001b[0m     )\n\u001b[0;32m    702\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:408\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    409\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    410\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    411\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    412\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    413\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    414\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    415\u001b[0m )\n\u001b[0;32m    416\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llama\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:322\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# reuse k, v, self_attention\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 322\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    324\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m (key_states, value_states) \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# repeat k/v heads if n_kv_heads < n_heads\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 11.00 GiB total capacity; 35.88 GiB already allocated; 0 bytes free; 36.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "df_data = get_iteration_chains_of_thoughts(\n",
    "    step=0,\n",
    "    llama=llama,\n",
    "    tokenizer=tokenizer,\n",
    "    df_data=df_data,\n",
    "    df_formatters=df_formatters,\n",
    "    batch_size=4\n",
    ")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdacaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama] *",
   "language": "python",
   "name": "conda-env-llama-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
